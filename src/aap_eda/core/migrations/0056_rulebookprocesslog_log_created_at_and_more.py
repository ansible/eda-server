# Generated by Django 4.2.16 on 2025-01-23 05:05

import logging

from django.db import migrations, models

from aap_eda.core.utils.rulebook_process_logs import (
    extract_datetime_and_message_from_log_entry,
)

logger = logging.getLogger(__name__)

CHUNK_SIZE = 1000


def up_log_entries(apps, schema_editor):
    rulebook_process_log_model = apps.get_model("core", "RulebookProcessLog")

    updated_entries = []
    total_records = 0

    for entry in rulebook_process_log_model.objects.iterator(
        chunk_size=CHUNK_SIZE
    ):
        dt, message = extract_datetime_and_message_from_log_entry(entry.log)
        entry.log_created_at = dt
        entry.log = message
        updated_entries.append(entry)

        # update per CHUNK_SIZE in case of large records in DB
        if len(updated_entries) >= CHUNK_SIZE:
            rulebook_process_log_model.objects.bulk_update(
                updated_entries, ["log", "log_created_at"]
            )
            total_records += len(updated_entries)
            logger.info(f"{total_records} entries are parsed.")
            updated_entries.clear()

    # update the last trunck
    if updated_entries:
        rulebook_process_log_model.objects.bulk_update(
            updated_entries, ["log", "log_created_at"]
        )
        total_records += len(updated_entries)

    logger.info(f"Totally {total_records} entries are parsed.")


def down_log_entries(apps, schema_editor):
    rulebook_process_log_model = apps.get_model("core", "RulebookProcessLog")

    updated_entries = []
    total_records = 0

    for entry in rulebook_process_log_model.objects.iterator(
        chunk_size=CHUNK_SIZE
    ):
        if entry.log_created_at:
            timestamp_str = entry.log_created_at.strftime(
                "%Y-%m-%d %H:%M:%S,%f"
            ).rstrip("0")
            message = f"{timestamp_str} {entry.log}"
            entry.log = message

            updated_entries.append(entry)

        # update per CHUNK_SIZE in case of large records in DB
        if len(updated_entries) >= CHUNK_SIZE:
            rulebook_process_log_model.objects.bulk_update(
                updated_entries, ["log"]
            )
            total_records += len(updated_entries)
            logger.info(f"{total_records} entries are reversed.")
            updated_entries = []

    # update the last trunck
    if updated_entries:
        rulebook_process_log_model.objects.bulk_update(
            updated_entries, ["log"]
        )
        total_records += len(updated_entries)

    logger.info(f"Totally {total_records} entries are reversed.")


class Migration(migrations.Migration):
    dependencies = [
        ("core", "0055_activation_created_by_activation_modified_by_and_more"),
    ]

    operations = [
        migrations.AddField(
            model_name="rulebookprocesslog",
            name="log_created_at",
            field=models.DateTimeField(null=True),
        ),
        migrations.RunPython(up_log_entries, down_log_entries),
    ]
